# Big O Notation
- Big O notation is a mathematical notation used to describe the upper bound of an algorithm's time complexity or space complexity. It provides a way to express how the runtime or memory usage of an algorithm grows as the input size increases.
- When analysing an algorithm, we focus on the least significant terms and ignore constant factors, as they do not affect the growth rate of the algorithm. For example, if an algorithm has a time complexity of O(2n + 3), we simplify it to O(n) because the linear term dominates the growth as n increases.
- The worst-case time complexity of an algorithm is the maximum amount of time it takes to complete for any input of size n. This is often what we refer to when we talk about Big O notation, as it provides a guarantee on the upper limit of the algorithm's performance.

## Orders of Growth
- **O(1)**: `Constant` time complexity. The algorithm takes the same amount of time regardless of the input size.
- **O(log n)**: `Logarithmic` time complexity. The algorithm's runtime grows logarithmically as the input size increases.
- **O(n)**: `Linear` time complexity. The algorithm's runtime grows linearly with the input size.
- **O(n log n)**: `Linearithmic` time complexity. The algorithm's runtime grows faster than linear but slower than quadratic.
- **O(n^2)**: `Quadratic` time complexity. The algorithm's runtime grows quadratically with the input size.
- **O(n^3)**: `Cubic` time complexity. The algorithm's runtime grows cubically with the input size.
- **O(2^n)**: `Exponential` time complexity. The algorithm's runtime doubles with each additional input element.
- **O(n!)**: `Factorial` time complexity. The algorithm's runtime grows factorially with the input size.

- When analysing the efficiency of an algorithm, we often focus on the step that has the highest order of growth, as it will dominate the overall time complexity. In this example, the `exampleAlgorithm` function has a time complexity of O(n) because the loop iterates through each element of the input array, while the other operations are constant time (O(1)).
```js
function exampleAlgorithm(arr) {
    let sum = 0;
    for (let i = 0; i < arr.length; i++) { // O(n)
        sum += arr[i]; // O(1)
    }
    return sum; // O(1)
}
```

## O(n) - Linear Time Complexity
- An algorithm has a linear time complexity of O(n) if its runtime grows linearly with the input size. This means that if the input size doubles, the runtime also doubles.

```python
def linear_search(arr, target):
    for i in range(len(arr)):
        if arr[i] == target:
            return i
    return -1
```
- In this example, the `linear_search` function iterates through each element of the array `arr` to find the `target`. The time complexity is O(n) because in the worst case, it may have to check all n elements.

## O(1) - Constant Time Complexity
- An algorithm has a constant time complexity of O(1) if its runtime does not depend on the input size. This means that the algorithm takes the same amount of time to execute regardless of the size of the input.

```rust
fn get_first_element(arr: &[i32]) -> Option<i32> {
    if arr.is_empty() {
        None
    } else {
        Some(arr[0])
    }
}
```
- In this example, the `get_first_element` function retrieves the first element of the array `arr`. The time complexity is O(1) because it performs a constant number of operations, regardless of the size of the input array.